{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models with Real World dataset\n",
    "\n",
    "In this file, we used all five datasets provided in the paper, some of which include 2-3 sub-datasets. We replicated DDGroup and implemented our own version of GBM for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Importing libraries](#i-importing-libraries-&-data-check)\n",
    "2. [Baseline](#II-Baseline)\n",
    "3. [DDGroup](#III-DDGroup)\n",
    "4. [GBM](#iv-gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Importing libraries & Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from utils.data_loader import *\n",
    "from utils.methods import *\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.methods import *\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.methods import *\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dutch_drinking_inh (12121, 16)\n",
      "Dutch_drinking_wm (12131, 16)\n",
      "Dutch_drinking_sha (12098, 16)\n",
      "Brazil_health_heart (7728, 6)\n",
      "Brazil_health_stroke (9675, 6)\n",
      "Korea_grip (1022, 11)\n",
      "China_glucose_women2 (4568, 11)\n",
      "China_glucose_men2 (4360, 11)\n",
      "Spain_Hair (529, 5)\n",
      "China_HIV (2410, 27)\n"
     ]
    }
   ],
   "source": [
    "# Data check \n",
    "\n",
    "dir_data = 'data/'\n",
    "\n",
    "# List of datasets to process\n",
    "names_data = ['Dutch_drinking_inh', 'Dutch_drinking_wm', 'Dutch_drinking_sha', 'Brazil_health_heart', \n",
    "              'Brazil_health_stroke', 'Korea_grip', 'China_glucose_women2', 'China_glucose_men2', \n",
    "              'Spain_Hair', 'China_HIV']\n",
    "\n",
    "# Loop through all datasets and calculate MSE\n",
    "for name_data in names_data:\n",
    "    # Load data\n",
    "    X, Y, names_covariates = load_regr_data(name_data, dir_data)\n",
    "    Y = Y.astype(np.float)\n",
    "    \n",
    "    print(name_data, X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Dutch_drinking_inh\n",
      "Validation MSE: 0.6242\n",
      "Test MSE: 0.6121\n",
      "--------------------------------------------------\n",
      "Dataset: Dutch_drinking_wm\n",
      "Validation MSE: 0.7109\n",
      "Test MSE: 0.8090\n",
      "--------------------------------------------------\n",
      "Dataset: Dutch_drinking_sha\n",
      "Validation MSE: 0.6327\n",
      "Test MSE: 0.5585\n",
      "--------------------------------------------------\n",
      "Dataset: Brazil_health_heart\n",
      "Validation MSE: 0.6144\n",
      "Test MSE: 0.5944\n",
      "--------------------------------------------------\n",
      "Dataset: Brazil_health_stroke\n",
      "Validation MSE: 0.3783\n",
      "Test MSE: 0.4318\n",
      "--------------------------------------------------\n",
      "Dataset: Korea_grip\n",
      "Validation MSE: 0.7236\n",
      "Test MSE: 0.8053\n",
      "--------------------------------------------------\n",
      "Dataset: China_glucose_women2\n",
      "Validation MSE: 0.8354\n",
      "Test MSE: 1.0999\n",
      "--------------------------------------------------\n",
      "Dataset: China_glucose_men2\n",
      "Validation MSE: 0.9106\n",
      "Test MSE: 0.9698\n",
      "--------------------------------------------------\n",
      "Dataset: Spain_Hair\n",
      "Validation MSE: 0.9183\n",
      "Test MSE: 0.7985\n",
      "--------------------------------------------------\n",
      "Dataset: China_HIV\n",
      "Validation MSE: 0.8710\n",
      "Test MSE: 0.9364\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Directory where data is stored\n",
    "dir_data = 'data/'\n",
    "\n",
    "# List of datasets to process\n",
    "names_data = ['Dutch_drinking_inh', 'Dutch_drinking_wm', 'Dutch_drinking_sha', 'Brazil_health_heart', \n",
    "              'Brazil_health_stroke', 'Korea_grip', 'China_glucose_women2', 'China_glucose_men2', \n",
    "              'Spain_Hair', 'China_HIV']\n",
    "\n",
    "# Loop through all datasets and calculate MSE\n",
    "for name_data in names_data:\n",
    "    # Load data\n",
    "    X, Y, names_covariates = load_regr_data(name_data, dir_data)\n",
    "    Y = Y.astype(np.float)\n",
    "\n",
    "    # Center the features (remove mean)\n",
    "    X -= np.mean(X, axis=0)\n",
    "\n",
    "    # Add bias term (column of ones)\n",
    "    n = len(Y)\n",
    "    X = np.append(X, np.ones((n, 1)), axis=1)\n",
    "\n",
    "    # Standardize Y values\n",
    "    Y = StandardScaler().fit_transform(Y.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    # Split data into training (50%) and test (50%)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=101)\n",
    "\n",
    "    # Further split the test set into test (60% of test) and validation (40% of test)\n",
    "    X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.4, random_state=101)\n",
    "\n",
    "    # Initialize the Linear Regression model\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Train the model on the training set\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    Y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # Predict on the test set\n",
    "    Y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the Mean Squared Error on validation set\n",
    "    mse_val = mean_squared_error(Y_val, Y_val_pred)\n",
    "\n",
    "    # Calculate the Mean Squared Error on test set\n",
    "    mse_test = mean_squared_error(Y_test, Y_test_pred)\n",
    "\n",
    "    # Output the results\n",
    "    print(f\"Dataset: {name_data}\")\n",
    "    print(f\"Validation MSE: {mse_val:.4f}\")\n",
    "    print(f\"Test MSE: {mse_test:.4f}\")\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. DDGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Dutch_drinking_inh\n",
      "Validation MSE: 0.5225\n",
      "Test MSE for points in the selected region: 0.5319\n",
      "----------------------------------------\n",
      "Processing dataset: Dutch_drinking_wm\n",
      "Validation MSE: 0.6709\n",
      "Test MSE for points in the selected region: 0.7211\n",
      "----------------------------------------\n",
      "Processing dataset: Dutch_drinking_sha\n",
      "Validation MSE: 0.5451\n",
      "Test MSE for points in the selected region: 0.4657\n",
      "----------------------------------------\n",
      "Processing dataset: Brazil_health_heart\n",
      "No points in the validation set are included in the selected region.\n",
      "Test MSE for points in the selected region: 0.5045\n",
      "----------------------------------------\n",
      "Processing dataset: Brazil_health_stroke\n",
      "Validation MSE: 0.2746\n",
      "Test MSE for points in the selected region: 0.0180\n",
      "----------------------------------------\n",
      "Processing dataset: Korea_grip\n",
      "Validation MSE: 0.7951\n",
      "Test MSE for points in the selected region: 0.8125\n",
      "----------------------------------------\n",
      "Processing dataset: China_glucose_women2\n",
      "Validation MSE: 0.0281\n",
      "Test MSE for points in the selected region: 0.5422\n",
      "----------------------------------------\n",
      "Processing dataset: China_glucose_men2\n",
      "Validation MSE: 0.8499\n",
      "Test MSE for points in the selected region: 0.8931\n",
      "----------------------------------------\n",
      "Processing dataset: Spain_Hair\n",
      "Validation MSE: 2.7736\n",
      "Test MSE for points in the selected region: 1.6342\n",
      "----------------------------------------\n",
      "Processing dataset: China_HIV\n",
      "Validation MSE: 0.5120\n",
      "Test MSE for points in the selected region: 0.5489\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of dataset names\n",
    "datasets = ['Dutch_drinking_inh', 'Dutch_drinking_wm', 'Dutch_drinking_sha', 'Brazil_health_heart', \n",
    "            'Brazil_health_stroke', 'Korea_grip', 'China_glucose_women2', 'China_glucose_men2', \n",
    "            'Spain_Hair', 'China_HIV']\n",
    "\n",
    "dir_data = 'data/'\n",
    "\n",
    "# Loop over all datasets\n",
    "for name_data in datasets:\n",
    "    print(f\"Processing dataset: {name_data}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    X, Y, names_covariates = load_regr_data(name_data, dir_data)\n",
    "\n",
    "    # Preprocessing (adding bias and standardizing Y)\n",
    "    n = len(Y)\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X = np.append(X, np.ones((n, 1)), axis=1)\n",
    "    Y = StandardScaler().fit_transform(Y.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    # Split into training, validation, and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=101)\n",
    "    X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.3, random_state=101)\n",
    "\n",
    "    # Select a core subset using DDGroup\n",
    "    n_core = int(len(Y_train) / 10)  # Set the core size (5% of training data)\n",
    "    X_core, Y_core = neighbor_core(X_train, Y_train, n_core)\n",
    "\n",
    "    # Fit a linear model to the core set\n",
    "    beta_hat, min_eig, s_hat = core_fit(X_core, Y_core)\n",
    "\n",
    "    # Determine valid regions using residual-based labeling\n",
    "    B = np.column_stack([np.min(X_train, axis=0), np.max(X_train, axis=0)])\n",
    "    labels = hard_grow_labels(X_train, Y_train, 0.05, s_hat, min_eig, n_core, beta_hat)\n",
    "\n",
    "    # Grow the region based on the core set labels (excluding bias term)\n",
    "    R_hat = hard_grow_region(X_train[:, :-1], labels, B[:-1])\n",
    "\n",
    "    # Evaluate the region on the validation set (excluding bias term)\n",
    "    ind_val = in_box(X_val[:, :-1], R_hat)\n",
    "    X_val_incl = X_val[ind_val]\n",
    "    Y_val_incl = Y_val[ind_val]\n",
    "\n",
    "    # Calculate validation MSE\n",
    "    if len(X_val_incl) > 0:\n",
    "        mse_val = mse(Y_val_incl, X_val_incl @ beta_hat)\n",
    "        print(f\"Validation MSE: {mse_val:.4f}\")\n",
    "    else:\n",
    "        print(\"No points in the validation set are included in the selected region.\")\n",
    "\n",
    "    # Evaluate the region on the test set (excluding bias term)\n",
    "    ind_test = in_box(X_test[:, :-1], R_hat)\n",
    "    X_test_incl = X_test[ind_test]\n",
    "    Y_test_incl = Y_test[ind_test]\n",
    "\n",
    "    # Calculate test MSE for points in the coreset region\n",
    "    if len(X_test_incl) > 0:\n",
    "        mse_test = mse(Y_test_incl, X_test_incl @ beta_hat)\n",
    "        print(f\"Test MSE for points in the selected region: {mse_test:.4f}\")\n",
    "    else:\n",
    "        print(\"No points in the test set are included in the selected region.\")\n",
    "\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Dutch_drinking_inh\n",
      "Validation MSE: 0.7695\n",
      "Test MSE for points in the selected region: 0.8217\n",
      "----------------------------------------\n",
      "Processing dataset: Dutch_drinking_wm\n",
      "Validation MSE: 0.7946\n",
      "Test MSE for points in the selected region: 0.8314\n",
      "----------------------------------------\n",
      "Processing dataset: Dutch_drinking_sha\n",
      "Validation MSE: 0.7764\n",
      "Test MSE for points in the selected region: 0.7183\n",
      "----------------------------------------\n",
      "Processing dataset: Brazil_health_heart\n",
      "No points in the validation set are included in the selected region.\n",
      "Test MSE for points in the selected region: 0.4167\n",
      "----------------------------------------\n",
      "Processing dataset: Brazil_health_stroke\n",
      "Validation MSE: 0.1378\n",
      "Test MSE for points in the selected region: 0.2029\n",
      "----------------------------------------\n",
      "Processing dataset: Korea_grip\n",
      "Validation MSE: 0.8124\n",
      "Test MSE for points in the selected region: 1.1701\n",
      "----------------------------------------\n",
      "Processing dataset: China_glucose_women2\n",
      "Validation MSE: 0.1249\n",
      "Test MSE for points in the selected region: 0.5935\n",
      "----------------------------------------\n",
      "Processing dataset: China_glucose_men2\n",
      "Validation MSE: 0.9033\n",
      "Test MSE for points in the selected region: 0.9888\n",
      "----------------------------------------\n",
      "Processing dataset: Spain_Hair\n",
      "Validation MSE: 1.2808\n",
      "Test MSE for points in the selected region: 0.8966\n",
      "----------------------------------------\n",
      "Processing dataset: China_HIV\n",
      "Validation MSE: 0.6659\n",
      "Test MSE for points in the selected region: 0.6729\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of dataset names\n",
    "datasets = ['Dutch_drinking_inh', 'Dutch_drinking_wm', 'Dutch_drinking_sha', 'Brazil_health_heart', \n",
    "            'Brazil_health_stroke', 'Korea_grip', 'China_glucose_women2', 'China_glucose_men2', \n",
    "            'Spain_Hair', 'China_HIV']\n",
    "\n",
    "dir_data = 'data/'\n",
    "\n",
    "# Loop over all datasets\n",
    "for name_data in datasets:\n",
    "    print(f\"Processing dataset: {name_data}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    X, Y, names_covariates = load_regr_data(name_data, dir_data)\n",
    "\n",
    "    # Preprocessing (adding bias and standardizing Y)\n",
    "    n = len(Y)\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X = np.append(X, np.ones((n, 1)), axis=1)\n",
    "    Y = StandardScaler().fit_transform(Y.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    # Split into training, validation, and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=101)\n",
    "    X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.3, random_state=101)\n",
    "\n",
    "    # Train a GBM model to capture regions\n",
    "    gbm_model = GradientBoostingRegressor(\n",
    "        n_estimators=30,\n",
    "        max_depth=1,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        random_state=40,\n",
    "        loss='huber'\n",
    "    )\n",
    "    gbm_model.fit(X_train, Y_train)\n",
    "\n",
    "    # Extract leaf regions for the training data\n",
    "    gbm_regions_train = gbm_model.apply(X_train)\n",
    "\n",
    "    # Extract the most frequent leaf index (mode) for each sample across all trees\n",
    "    most_frequent_region_train = stats.mode(gbm_regions_train, axis=1, keepdims=False).mode.flatten()\n",
    "\n",
    "    # Select core points based on the most frequent region\n",
    "    core_indices_train = most_frequent_region_train == np.unique(most_frequent_region_train)[0]\n",
    "    X_core_train = X_train[core_indices_train]\n",
    "    Y_core_train = Y_train[core_indices_train]\n",
    "\n",
    "    # Refine core points selection by taking only points with smallest residuals\n",
    "    residuals = np.abs(gbm_model.predict(X_core_train) - Y_core_train)  # Fixed this line\n",
    "    n_core = int(0.1 * len(X_train))  # Select 10% of the original training data size\n",
    "    smallest_residual_indices = np.argsort(residuals)[:n_core]\n",
    "    X_core_train = X_core_train[smallest_residual_indices]\n",
    "    Y_core_train = Y_core_train[smallest_residual_indices]\n",
    "\n",
    "    # Fit a Linear Model to the core set\n",
    "    beta_hat, min_eig, s_hat = core_fit(X_core_train, Y_core_train)\n",
    "\n",
    "    # Determine valid regions using residual-based labeling\n",
    "    B = np.column_stack([np.min(X_train, axis=0), np.max(X_train, axis=0)])\n",
    "    labels = hard_grow_labels(X_train, Y_train, 0.05, s_hat, min_eig, len(X_core_train), beta_hat)\n",
    "\n",
    "    # Grow the region based on the core set labels (excluding bias term)\n",
    "    R_hat = hard_grow_region(X_train[:, :-1], labels, B[:-1])\n",
    "\n",
    "    # Evaluate the region on the validation set (excluding bias term)\n",
    "    ind_val = in_box(X_val[:, :-1], R_hat)\n",
    "    X_val_incl = X_val[ind_val]\n",
    "    Y_val_incl = Y_val[ind_val]\n",
    "\n",
    "    # Calculate validation MSE\n",
    "    if len(X_val_incl) > 0:\n",
    "        mse_val = mean_squared_error(Y_val_incl, X_val_incl @ beta_hat)\n",
    "        print(f\"Validation MSE: {mse_val:.4f}\")\n",
    "    else:\n",
    "        print(\"No points in the validation set are included in the selected region.\")\n",
    "\n",
    "    # Evaluate the region on the test set (excluding bias term)\n",
    "    ind_test = in_box(X_test[:, :-1], R_hat)\n",
    "    X_test_incl = X_test[ind_test]\n",
    "    Y_test_incl = Y_test[ind_test]\n",
    "\n",
    "    # Calculate test MSE for points in the coreset region\n",
    "    if len(X_test_incl) > 0:\n",
    "        mse_test = mean_squared_error(Y_test_incl, X_test_incl @ beta_hat)\n",
    "        print(f\"Test MSE for points in the selected region: {mse_test:.4f}\")\n",
    "    else:\n",
    "        print(\"No points in the test set are included in the selected region.\")\n",
    "\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
